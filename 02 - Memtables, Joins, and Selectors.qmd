---
title: 'Memtables, Joins and, Selectors, oh my!'
jupyter: python3
---



## Memtables

`ibis.memtable` is a convenient way to take data you already have in-memory and use it within one of the Ibis backends.

**Note**: for very large tables, there are performance implications depending on the backend you use.

```{python}
import ibis
import pandas as pd
import pyarrow as pa
```

```{python}
ibis.options.interactive = True
```

```{python}
plain_dict = {
    "name": ["Gil", "Phillip", "Jim", "Krisztián", "Wes"],
    "committed": [176, 2680, 158, 288, 387],
    "aliases": [
        "Schemata Eldritch",
        "Chuckles",
        "Minnesota Fats",
        "Szűcs",
        "Goldeneye",
    ],
}
pat = pa.Table.from_pydict(
    {
        "name": ["Gil", "Phillip", "Jim", "Krisztián", "Wes"],
        "pickleCount": [9.22, 5.123, 6.242, 22.47, None],
    },
)

df = pd.DataFrame(
    {
        "name": ["Gil", "Phillip", "Jim", "Krisztián", "Wes"],
        "committed": [True, True, True, True, True],
        "first_commit": [
            "b0f4f44a1",
            "2cd935599",
            "67037d099",
            "3324d1f2d",
            "84d043583",
        ],
    }
)
```

You can construct a memtable from a dictionary, a Pandas DataFrame, or a PyArrow Table

```{python}
t1 = ibis.memtable(plain_dict)
t2 = ibis.memtable(pat)
t3 = ibis.memtable(df)
```

```{python}
t1
```

```{python}
print(pat.schema)
t2
```

```{python}
print(df.dtypes)
t3
```

If a `memtable` is created from a plain dictionary (or if a `pandas.Series` is of `object` dtype), `ibis` will infer the dtype. Well-typed columns will have their datatypes preserved (modulo some possible small name differences, e.g. `uint` vs. `uinteger`)

```{python}
t2
```

## Joins

Many interesting datasets (and also boring ones!) are stored in separate tables.  To perform analysis on these data, we have to join the separate tables togther.

Let's start with some Ibis maintainer data:

```{python}
t1.join(t2, "name")
```

By default, we perform an inner join.  If the join column has the same name in both tables, you can pass in the column name as a string.

If they are different, or you care to specify it, you can also provide the columns more explicitly:

```{python}
t1.join(t2, t1.name == t2.name)
```

### Name collisions

If the tables share more than just the join column in common, Ibis will rename columns as needed:

```{python}
t1.join(t3, "name")
```

But you can also specify different naming schemes:

```{python}
t1.join(t3, "name", lname="{name}_left")
```

```{python}
t1.join(t3, "name", rname="right_{name}")
```

### Other Joins

The default join in Ibis is an inner-join, but you can specify several other types of join using the `how` keyword:

```{python}
t1.join(t3, t1.name == t3.first_commit, how="outer")
```

There are also join-specific methods for each type of join:

```{python}
t1.outer_join(t3, t1.name == t3.first_commit)
```

### Chaining Joins

You can also chain multiple joins, for instance:

```{python}
t1.join(t2, "name").join(t3, "name")
```

## Deferred operator

Let's consider again the output of one of the joins above:

```{python}
t1.join(t3, "name")
```

Let's say we want to filter on the contents of the `committed_right` column (admittedly, a bit silly, since it is all `True`) -- how do we refer to that column?

We've so far used the `table_variable.column_name` construction, so let's try that:

```{python}
t1.join(t3, "name").filter([t1.committed_right == True])
```

Maybe a lambda function?

```{python}
t1.join(t3, "name").filter(lambda x: x.committed_right == True)
```

A lambda function works, but it can be a little awkward to constantly write lambda functions.  To help with this, Ibis provides a deferred operator.

```{python}
from ibis import _
```

The `_` serves as a shorthand for representing the entire chained expression up to the most recent chained method call.

---

**Note**: Yes, this does collide with the convention of having `_` refer to the output of the most recent REPL command.  If this is a deal-breaker you can assign it to a different name, e.g.

```python
from ibis import deferred as C
```

---

Instead of writing a lambda function (not that those are bad!), we can write:

```{python}
t1.join(t3, "name").filter(_.committed_right == True)
```

And we can continue chaining more methods and use the underscore to simplify our typing.

```{python}
(
    t1.join(t3, "name").rename(has_committed="committed_right")
    # _ is the table with the renamed column
    .filter(_.has_committed == True)
    # _ is the relnamed, filtered table
    .mutate(commit_percent=_.committed / _.committed.sum() * 100)
    # _ is the renamed, filtered, mutated table
    .order_by(_.commit_percent.desc())
)
```

We think this is very convenient!  And its use isn't limited to chained methods -- it can also be convenient in simpler calls:

```{python}
a_long_name_for_a_table = ibis.memtable(plain_dict)
a_long_name_for_a_table
```

```{python}
a_long_name_for_a_table[a_long_name_for_a_table.aliases.startswith("S")]
```

```{python}
a_long_name_for_a_table[_.aliases.startswith("S")]
```

## More interesting data

For this section, we'll use some of Ibis' built-in example datasets, specifically, some IMDB data.

---

**Note**: the full data for both of these tables is available in `ibis.examples.imdb_title_ratings` and `ibis.examples.imdb_title_basics`, but we're not using those in-person to avoid everyone downloading the same 250mb file at once.

---

```{python}
from pathlib import Path

filenames = [
    "imdb_basics_sample_5.parquet",
    "imdb_ratings.parquet",
]

folder = Path("imdb_smol")
folder.mkdir(exist_ok=True)
```

```{python}
for filename in filenames:
    path = folder / filename
    if not path.exists():
        import urllib.request

        urllib.request.urlretrieve(
            f"https://storage.googleapis.com/ibis-tutorial-data/imdb_smol/{filename}",
            path,
        )
```

```{python}
!ls
```

### Parquet loading

In the previous examples we used a pre-existing DuckDB database, and some in-memory tables. Another common pattern is that you have a few parquet files you want to work with. We can load those in to an in-memory DuckDB connection.  (Note that "in-memory" here just means ephemeral, DuckDB is still very happy to operate on as much data as your hard drive can hold)

```{python}
con = ibis.duckdb.connect()
```

```{python}
basics = con.read_parquet(
    "imdb_smol/imdb_basics_sample_5.parquet", table_name="imdb_title_basics"
)
```

```{python}
ratings = con.read_parquet(
    "imdb_smol/imdb_ratings.parquet", table_name="imdb_title_ratings"
)
```

The `read_parquet` method returns an Ibis table that points to the to-be-ingested `parquet` file. 

`read_parquet` also registers the table with DuckDB (or another backend), so you can also load the tables like we did for the `penguins` table in the previous notebook.

```{python}
basics = con.tables.imdb_title_basics  # this cell is redundant, just here for demonstration
```

```{python}
ratings = con.tables.imdb_title_ratings  # this cell is redundant, just here for demonstration
```

```{python}
#| scrolled: true
basics
```

## Exercises

### Exercise 1

Join `basics` with `ratings` on the `tconst` column.


#### Solution

```{python}
#| scrolled: true
%load solutions/nb02-ex01.py
```

### Exercise 2

Join `basics` with `ratings` on `tconst`, and select out only the `titleType`, `primaryTitle`, `numVotes`, `averageRating`, and `isAdult`  columns.


#### Solution

```{python}
%load solutions/nb02-ex02.py
```

### Exercise 3

Those `camelCase` column names aren't [PEP 8](https://peps.python.org/pep-0008/) compliant, and feel a bit clunky to use. Modify the above to change them to `snake_case` (for example, rename `titleType` to `title_type`).

There are two ways you might achieve this:

- Using the `Table.rename` method
- Or by modifying the `.select` used above to do the relabeling in one step.


#### Solution

```{python}
%load solutions/nb02-ex03-rename.py
```

```{python}
%load solutions/nb02-ex03-select.py
```

### Exercise 4

Using the above joined table, compute the 10 non-adult movies with the highest average rating having received at least 100,000 votes.


#### Solution

```{python}
%load solutions/nb02-ex04.py
```

## Expression portability

```{python}
import os

# this will only work if you have snowflake credentials set up in this environment variable
snowflake_con = ibis.connect(os.getenv("SNOWFLAKE_URL"))
```

Note that for demo purposes, we've preloaded these tables into our Snowflake account.
But if you wanted to add them to your Snowflake account, you could make use of `read_parquet` or similar!

```{python}
snowflake_con.list_tables(like="imdb")
```

```{python}
ibis.options.interactive = False
```

```{python}
expr = sol4.unbind()
```

```{python}
expr
```

```{python}
snowflake_con.execute(expr).head(10)
```

## Selectors

Selectors are a helper to concisely express operations across groups of columns -- let's dive in!

```{python}
from ibis import selectors as s
from ibis import _
```

We'll return to the penguins data we looked at in the first notebook.

```{python}
con = ibis.duckdb.connect("palmer_penguins.ddb", read_only=True)
```

```{python}
penguins = con.table("penguins")

penguins
```

Selectors let you select columns based on some criteria, for instance:

```{python}
penguins[s.numeric()]
```

```{python}
penguins[s.of_type("int64")]
```

```{python}
penguins[s.endswith("_mm")]
```

That's pretty useful already, but we can also use selectors to perform operations across the selected columns!

For instance, to compute the Z-score ($\frac{x - \mu}{\sigma}$) of the numeric columns in the penguins data, we can do the following (with help from our friend the `_`):

```{python}
penguins.mutate(s.across(s.numeric(), (_ - _.mean()) / _.std()))
```

And just like that, we've computed the z-score across every numeric column! 

Let's examine that line in a bit more detail, because there is a lot going on there.

We use `across`, which applies an operation across all columns matching some criteria.
In this example, the criteria is `numeric`, which is a selector that will grab all integer and floating-point columns.

Within the `across`, the `_` will stand in for each column matched by our selection criteria (`numeric`).

Wrapping it all up in a `mutate` call means we overwrite the values of the selected columns with the new computed values.


It might've been weird to compute the z-score of the `year`, though. The `year` column is typed as an `int64`, which makes sense, but we don't want to treat it as a measurement. 

What we want, is all the `numeric` columns _except_ for `year`.
That's not hard, because selectors are composable!

```{python}
penguins.mutate(s.across(s.numeric() & ~s.c("year"), (_ - _.mean()) / _.std()))
```

And we've compute the z-score for all of our penguins vital statistics and avoided mangling the year column.

### Exercise 5

We might want to ensure uniformity in our labeling.  Lowercase all of the string values across all of the string columns.


#### Solution

```{python}
%load solutions/nb02-ex05.py
```

### Exercise 6

We computed the z-score for a given column across every penguin on every island -- but we might instead want to compute those same statistics on a species-by-species basis.  Try to compute the same normalization, but where the computation takes place over each species of penguin individually.


#### Solution

```{python}
%load solutions/nb02-ex06.py
```

